user_input,retrieved_contexts,response,reference,context_recall,context_precision,faithfulness,answer_correctness
What are some automated tools and approaches for optimizing prompts in AI systems?,"['. Tools that aim to automate the whole prompt engineering workflow include Open‐ Prompt (Ding et al., 2021) and DSPy (Khattab et al., 2023). At a high level, you spec‐ ify the input and output formats, evaluation metrics, and evaluation data for your task. These prompt optimization tools automatically find a prompt or a chain of prompts that maximizes the evaluation metrics on the evaluation data. Functionally, these tools are similar to autoML (automated ML) tools that automatically find the optimal hyperparameters for classical ML models. A common approach to automating prompt generation is to use AI models. AI mod‐ els themselves are capable of writing prompts. 10 In its simplest form, you can ask a model to generate a prompt for your application, such as “Help me write a concise prompt for an application that grades college essays between 1 and 5”. You can also ask AI models to critique and improve your prompts or generate in-context exam‐ ples', '. We will also discuss methods for operationalizing these strategies through LLMOps, which stands as a critical component in the automation process. References • Introduction to Prompt Engineering: https://tinyurl.com/azu5ubma • Prompt Engineering Guide: https://www.promptingguide.ai/ • Awesome ChatGPT prompts: https://github.com/f/awesome-chatgpt-prompts • Best practices for prompt engineering with Open AI: https://help.openai.com/en/ articles/6654000-best-practices-for-prompt-engineering-with- the-openai-api • Azure OpenAI Service: Azure OpenAI | Microsoft Learn: https://learn.microsoft. com/en-us/azure/ai-services/openai/concepts/prompt-engineering • System Message Framework: https://learn.microsoft.com/en-us/azure/ ai-services/openai/concepts/system-message • Ethics in Prompt Engineering: https://promptengineeringai.in/ethics-of- prompt-engineering/#:~:text=Prompt%20engineering%20should%20 respect%20user,their%20data%20is%20being%20used', '. When we talk about ChatGPT and OpenAI models, prompts mainly refer to a piece of text in natural language or – as we saw for Codex models – in programming languages. Prompts are the only way users can control the output generated by those models. As such, there is no surprise in saying that the quality of the prompts used to engage the AI system in a conversation is key to determining its success.', '. The paradigm shift from human-designed prompts to Generative AI prompts will help users who want to avoid imagining prompts that are time-consuming and require tri- al-and-error approaches. This is yet another RAG in which an automated system will retrieve and parse documents (Step 1 above) to create prompts that will, in turn, be the input of another Generative AI model. Llama and GPT-4 will replace the human design process. This process requires a development investment but will free end users of the burden of finding the right prompt when human resources and budgets are scarce.', '. Looking ahead, techniques such as conditional prompting for personalization, causality prompting, deanonymization prompting, and AI debate hold promise for safer, more reliable applications. Let’s explore these prompting types briefly: • Conditional prompting involves structuring prompts to condition model outputs on provided user attributes and preferences, allowing generated content to be tailored for individual users. • Causality prompting aims to imbue generative AI with causal reasoning by crafting prompts that require inferring chains of cause and effect. This could enable systems to answer causal what-if questions and improve decision-making. • Approaches such as deanonymization prompting aim to surface potential re-identification risks by carefully prompting models to reconstruct redacted information. As capabilities grow, responsible prompting will be needed to probe dangers', 'Vertex AI – Generative AI Tools272 Here are some essential aspects of prompt design: • Clarity: Ensuring the prompt clearly conveys the desired task to the model • Specificity: Making the prompt specific to avoid ambiguous or overly generalized responses • Context: Providing enough context in the prompt to facilitate a more informed and relevant output • Formatting: Structuring the prompt in a manner that encourages the desired format of the response • Testing and iteration: Prompt design is often an iterative process involving testing various prompt strategies and fine-tuning them based on the outputs received • Ethical considerations: Design prompts that are ethical and avoid encouraging harmful, biased, or inappropriate responses from the model • Safety measures: Implementing safety measures such as using techniques to limit the model to safe and appropriate responses In practice, prompt design can involve a mixture of art and science, requiring both creative and analytical skills', '. Here are some tips on prompt writing: • Use descriptive inputs to frame the scenario, and explain the purpose of the prompt and what you are trying to achieve • Outline clear, specific expectations and guidelines • Specify what character or profession the AI should mimic, what tone of voice to use, and who the audience is • Give your own examples of what the result could look like • Provide references with URLs sourced from the internet for further examples We have collated some useful Zapier resources that will help you in your journey to becoming an AI-prompting expert: • https://zapier.com/blog/gpt-prompt/ • https://community.zapier.com/featured-articles-65/prompting- basics-get-quality-responses-when-interacting-with-ai-24314 • https://community.zapier.com/how-do-i-3/ai-voice-tone-when- creating-content-23182 The OpenAI API documentation also provides very useful information on best practices here: https:// platform.openai.com/docs/guides/gpt-best-practices', '. This leads to more accurate and relevant responses, turning a simple interaction into a robust tool for a variety of cloud-based applications. Whether it’s automating customer support, generating content, or analyzing data, the ability to fine-tune prompts is a game-changer, ensuring that the AI’s capabilities are fully leveraged in a controlled and purposeful manner. This chapter delves into the techniques for effective prompt engineering, offering strategies to refine interactions for optimal outcomes. This goes hand in hand with the ethical dimensions of prompt engineering, a topic of paramount importance in today’s tech landscape. It addresses the responsibility of developers to ensure that AI interactions are not only efficient and goal-oriented but also ethically sound and bias-free. Finally, the integration of prompt engineering with cloud solutions opens up a new realm of possibilities', 'Effective Prompt Engineering Techniques: Unlocking Wisdom Through AI122 • Instead of saying what not to provide, give alternative recommendations: Provide an alternative path if ChatGPT is unable to perform a task, such as in the following highlighted message: Example: System Message: Y ou are an AI nutrition consultant that provides nutrition consultation based on health and wellness goals of the customer Please note that any questions or inquiries beyond the scope of nutrition consultation will NOT be answered and instead will receive the response: “Sorry! This question falls outside my domain of expertise!” Customer: How do I invest in 401K? Nutrition AI Assistant: “Sorry! This question falls outside my domain of expertise!” • Provide example-based prompts: This helps the language model learn from specific instances and patterns. Start with a zero-shot, then a few-shot, and if neither of them works, then fine- tune the model', '. It’s a key skill in the field of AI, especially for those working on AI-powered chatbots, virtual assistants, content creation tools, and other applications that rely on NLP . Prompt content A “prompt” can potentially be broken down into several components, depending on the complexity of the task at hand. Here are common parts that constitute a well-structured prompt: • Instruction: This is the part of the prompt where you provide a clear directive or question to guide the AI’s response. The instruction should be explicit about the kind of information or the format of the answer you are expecting. An instruction can be as simple as “What is the capital of California?” or as complicated as a 10-page-long list of rules to be taken into consideration by the model. This detailed set of instructions can overlap with the prompt “context” discussed next. • Context: If the prompt is a follow-up or is seeking detailed information, providing context can be essential']","Automated tools and approaches for optimizing prompts include tools that search or generate prompts automatically (auto-prompting tools and prompt-optimization systems), using AI models to generate/critique/improve prompts or chains of prompts, and operational practices like iterative testing, evaluation metrics, and LLMOps to automate prompt selection and deployment.

Supporting details
- Tools that automate the whole prompt-engineering workflow include OpenPrompt and DSPy; these let you specify input/output formats, evaluation metrics, and evaluation data, and then automatically find a prompt or chain of prompts that maximizes evaluation metrics on the evaluation data—functionally similar to AutoML for hyperparameter tuning [1].
- A common automated approach is to use AI models themselves to write, critique, and improve prompts, or to generate in-context examples (e.g., “Help me write a concise prompt…”), and to create chains of prompts (generative AI replacing human-designed prompts) that feed into other models [1][4].
- Operationalizing prompt optimization involves LLMOps and iterative testing: specify evaluation metrics and data, test prompts (zero-shot, few-shot, then fine-tune if needed), and iterate based on outputs; best practices include clarity, specificity, context, formatting, testing and iteration, and ethical/safety considerations [2][8][9][6][10].
- RAG-style pipelines can be used: an automated system retrieves and parses documents to create prompts that become inputs to another generative model, reducing manual prompt design effort [4].
- Prompt-design techniques useful in optimization workflows include providing example-based prompts (zero-shot → few-shot → fine-tune), supplying alternatives when the model can’t perform a task, and structuring prompts (instruction, context, formatting) to guide models toward desired outputs [9][6][10].

References
[1] Ai Engineering, Page: 253
[2] Generative Ai For Cloud Solutions, Page: 150
[4] Transformers For Natural Language Processing And Computer Vision, Page: 649
[6] Definitive Guide To Google Vertex Ai, Page: 292
[8] Generative Ai For Cloud Solutions, Page: 131
[9] Generative Ai For Cloud Solutions, Page: 146
[10] Definitive Guide To Google Vertex Ai, Page: 292","Tools like OpenPrompt and DSPy automate prompt engineering by letting users declare input/output formats, constraints, and evaluation metrics in code. These frameworks then use AI models themselves to generate, critique, and refine prompts, often in a closed-loop process driven by validation data.

Beyond these, there’s a growing ecosystem of automatic prompt optimisation (APO) methods. Some systems treat prompt search as a black-box optimisation problem, using techniques like Bayesian optimisation or bandit algorithms to explore many prompt variants and keep the best-performing ones. Others integrate with evaluation harnesses (e.g. task-specific test sets, rubric-based LLM judges, or human-in-the-loop feedback) to score prompts at scale.

More advanced research approaches include:

Promptbreeder, which uses evolutionary strategies (mutation, crossover, selection) to “breed” better prompts over many generations.

TextGrad and related “gradient-free” methods, which treat the model as a differentiable-ish black box and use its own textual feedback (or score signals) to iteratively update the prompt.

Self-refinement / self-critique loops, where an LLM generates an answer, critiques it using another prompt (or another model), and then updates the original prompt or instructions based on the critique.

In production settings, prompt optimisation is often combined with tooling and MLOps practices:

Experiment tracking and A/B testing of prompt variants across real traffic.

Dataset-driven prompt tuning, where prompts are optimised against a curated benchmark of real user queries and gold-standard outputs.

Guardrail and policy-aware optimisation, where prompts are automatically rejected or revised if they increase safety or compliance risks.

Finally, some systems blend prompt optimisation with lightweight parameter tuning (e.g. adapters, LoRA, or preference optimisation) so that the line between “prompt engineering” and “model fine-tuning” becomes more of a continuum. This allows organisations to start with automated prompt search and, where needed, progress towards training small task-specific adapters while reusing the same evaluation and optimisation pipeline.",0.21428571428571427,0.8333333332916666,0.92,0.43691218343277927
How does Tree-of-Thought prompting work in LangChain?,"['. It occurs twice in the given answers (3 and 4). We should get the right response based on the vote; however, of the five responses we produced, three were wrong. This approach leverages the model’s ability to reason and utilize internal knowledge while reduc- ing the risk of outliers or incorrect information, by focusing on the most recurring answer, thus improving the overall reliability of the response given by the LLM. Tree-of-thought In Tree-of-Thought (ToT) prompting, we generate multiple problem-solving steps or approaches for a given prompt and then use the AI model to critique them. The critique will be based on the model’s judgment of the solution’s suitability to the problem. There is actually an implementation now of ToT in the LangChain experimental package; however, let’s walk through an instructive step-by-step example of implementing ToT using LangChain. First, we’ll define our four chain components with PromptTemplates', 'in an informed manner. Integrating Chroma or Weaviate plays a crucial role by offering a reservoir of contextual data, ensuring that tasks are always aligned with their predefined goals. Figure 6-4. BabyAGI’s agent architecture The plan-and-execute agent type does exist within LangChain, though it’s still experi‐ mental. Tree of Thoughts As the application of language models in problem-solving expands across diverse tasks, their inference method remains bound to token-level, linear processing. This approach, while effective in many contexts, is limited when faced with tasks that need advanced strategic foresight or where the initial decisions are crucial. The Tree of Thoughts (ToT) framework is a novel way to harness language models that goes beyond the conventional chain-of-thought prompting technique (Figure 6-5). The central premise of ToT is to enable exploration across coherent text chunks, termed thoughts', '10 Shunyu Y ao et al. “Tree of thoughts: Deliberate problem solving with large language models. ” arXiv preprint arXiv:2305.10601 (2023). 11 “Using tree-of-thought prompting to boost ChatGPT’s reasoning. ” Available at https://oreil.ly/a_Nos. solutions to the problem at hand. It then votes for the best solution and continues to the next step.10 Figure 6-18. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned. This method is tremendously helpful when needing to consider multiple paths, like when writing a story or coming up with creative ideas. A disadvantage of this method is that it requires many calls to the generative models, which slows the application significantly']","Tree-of-Thought (ToT) prompting in LangChain is an experimental approach that generates multiple intermediate ""thoughts"" (partial solutions), has the model critique or vote on them, keeps promising branches, prunes poor ones, and continues exploration to produce a final solution [1][3].

Supporting details:
- ToT expands on chain-of-thought by exploring a tree of coherent text chunks called thoughts, allowing the model to consider multiple solution paths and pursue promising branches rather than a single linear chain [2].
- The workflow typically: generate multiple problem-solving steps/approaches for a prompt, use the model to critique or vote on those solutions, keep the most recurring or highest-rated thoughts, prune low-rated ones, and iterate to the next step until a final answer emerges [1][3].
- LangChain provides an experimental implementation of ToT (an implementation exists in the LangChain experimental package) and examples showing how to build ToT with PromptTemplates and separate chain components [1].
- Benefits: improves reliability by focusing on recurring/high-rated answers and enables strategic exploration useful for tasks like story writing or creative ideation [1][3][2].
- Limitation: requires many calls to the generative model, which slows the application significantly [3].

References
[1] Generativeaiwithlangchain, Page: 273
[2] Promptengineeringforgenerativeai, Page: 279
[3] Handsonlargelanguagemodels, Page: 211","Tree-of-Thought (ToT) prompting gets an AI model to explore multiple reasoning paths instead of committing to a single chain of thought. At each step, it generates several candidate “thoughts” (partial solutions or strategies), evaluates them, prunes weak branches, and continues expanding the promising ones until it finds a good answer.

In LangChain, ToT isn’t a single magic class, but is typically built by composing multiple LLMChain (or LangGraph nodes) plus PromptTemplates into a search loop. A common pattern uses four main prompt templates:

a solution / expansion template to propose new “thoughts” or next steps from the current state,

an evaluation template to rate each candidate (e.g. correctness, feasibility, safety),

a reasoning template to let the model explain its evaluation or refine the line of thought,

a ranking / selection template to compare candidates and pick which branches to continue exploring.

Around those chains, you write control logic (in Python) that:

maintains the tree (nodes = states / partial solutions, edges = transitions / thoughts),

chooses a search strategy (breadth-first, depth-first, beam search, etc.),

repeatedly calls the expansion and evaluation chains to grow and prune branches,

stops when a node satisfies some stopping criterion (e.g. high score, consistent answer, or max depth).

In practice, LangChain’s extras help glue this together in a more “production-y” way:

Callbacks and tracing (e.g. LangSmith) to see the whole tree expansion and debugging traces.

Memory or state objects to carry intermediate context between thoughts.

Tool calling / agents combined with ToT, where different branches might invoke different tools before being scored.

The end result is that, rather than a single linear chain of thought, LangChain orchestrates a search over many possible reasoning paths, using the model both as a generator of candidate thoughts and as a critic that scores and ranks them. It’s basically “model as planner and reviewer”, wrapped in a tree search loop.",0.4,0.9999999999666667,0.9523809523809523,0.5917491676830737
What are common use cases for the PanResponder API in React Native?,"['with PanResponder 216 Use cases for the PanResponder API 217 ■ Example of using PanResponder 218 10 Implementing iOS‑specific components and APIs 222 10.1 Targeting platform-specific code 223 iOS and Android file extensions 223 ■ Detecting the platform using the Platform API 224 10.2 DatePickerIOS 226 Example of using DatePickerIOS 226 10.3 Using PickerIOS to work with lists of values 228 Example of using PickerIOS 230 10.4 Using ProgressViewIOS to show loading indicators 231 Use cases for ProgressViewIOS 232 ■ Example of using ProgressViewIOS 232 10.5 Using SegmentedControlIOS to create horizontal tab bars 233 Use cases for SegmentedControlIOS 234 ■ Example of using SegmentedControlIOS 234', 'React Native in Action', '217Getting information about touch and gesture events with PanResponder 9.9.1 Use cases for the PanResponder API Because the fundamental functionality of PanResponder is to determine the current touches happening on the user’s device, the use cases are unlimited. In my experience, I’ve used this API often to do things like the following: ¡ Create a swipeable stack of cards where an item is removed from the stack when swiped out of view (think Tinder) ¡ Create an animatable overlay that the user can close by clicking a button or move out of view by swiping down ¡ Give the user the ability to rearrange items in a list by pressing part of a list item and moving to the desired location The use cases for PanResponder are many, but the most apparent and frequently used let the user move items around in the UI based on their press/swipe position', '220 chapter 9 Implementing cross‑platform APIs this._handlePanResponderMove = this._handlePanResponderMove.bind(this) this._handlePanResponderRelease = this._handlePanResponderRelease.bind(this) } componentWillMount () { this._panResponder = PanResponder.create({ onStartShouldSetPanResponder: () => true, onPanResponderMove: this._handlePanResponderMove, onPanResponderRelease: this._handlePanResponderRelease }) } _handlePanResponderMove (evt, gestureState) { let ydiff = gestureState.y0 - gestureState.moveY let xdiff = gestureState.x0 - gestureState.moveX this.setState({ position: { y: this.state.oPosition.y - ydiff, x: this.state.oPosition.x - xdiff } }) } _handlePanResponderRelease () { this.setState({ oPosition: this.state.position }) } render () { return ( <View style={styles.container}> <Text style={styles.positionDisplay}> x: {this.state.position.x} y:{this.state.position.y} </Text> <View {...this._panResponder.panHandlers} style={[styles.box, { marginLeft: this.state.position.x,', ""219Getting information about touch and gesture events with PanResponder Listing 9.10 Using PanResponder to create a draggable element import React, { Component } from 'react' import { Dimensions, TouchableHighlight, PanResponder, TextInput, View, Text, StyleSheet } from 'react-native' const { width, height } = Dimensions.get('window') let styles = {} class App extends Component { constructor () { super() this.state = { oPosition: { x: (width / 2) - 100, y: (height / 2) - 100, }, position: { x: (width / 2) - 100, y: (height / 2) - 100, }, } Imports Dimensions, PanResponder, and everything else needed for this component Stores the window width and height in variables for later use Creates an object called oPosition to store the original square position x and y axes to center the square, and stores it into the state Creates an object called position to store the actual square position x and y axes to center the square, and stores it into the state Each configuration option is supplied"", '. onPanResponderTerminationRequest Gets called when something else wants to become responder. onPanResponderRelease Gets called when the touch has been released. onPanResponderTerminate This responder has been taken by another one.']","PanResponder use cases: common uses include allowing users to move UI elements with touch — for example, creating swipeable card stacks, draggable/animatable overlays that can be swiped away, and letting users rearrange list items by press-and-drag [1].

Supporting details:
- The PanResponder API’s fundamental purpose is to determine current touches on the device, so it’s used whenever you need the user to press, swipe, or drag UI elements [1].
- Example scenarios given: creating a Tinder‑style swipeable stack of cards where an item is removed when swiped out of view; creating an animatable overlay that can be closed by a button or moved out of view by swiping down; enabling rearrangement of list items by pressing part of a list item and dragging to a new location [1].
- The chapter notes that the most apparent and frequently used cases let the user move items around in the UI based on their press/swipe position [1].

References
[1] React Native In Action, Page: 217","PanResponder is a low-level gesture system in React Native that lets you track and respond to touch movements frame-by-frame. It’s often used when you need fine-grained control over gestures rather than relying on higher-level components.

You already mentioned some core use cases:

Swipeable card stacks (e.g. Tinder-style left/right swipes)

Dismissible / swipeable overlays and panels

Drag-to-reorder items in a list

Those are spot on. You can widen the scope to include:

Draggable UI elements in general

Dragging map markers, chips, floating buttons, or custom handles.

Moving shapes around in a canvas or game-like interface.

Custom sliders and knobs

Volume or brightness sliders that don’t fit the standard Slider component.

Rotary controls (e.g. a circular knob or dial) where you convert gesture movement to an angle.

Bottom sheets and drawers

Pull-up / pull-down panels that track the user’s finger and then “snap” open or closed based on final position and velocity.

Side drawers that can be dragged partially out and then released.

Free-form drawing and sketching

Capturing finger paths to draw lines, signatures, or annotations.

Visualising touch trails for debugging or custom visual effects.

Interactive animations tied to gestures

Mapping gestureState.dx / dy directly to Animated.ValueXY to create smooth, physics-based interactions.

Using the final velocity to trigger a fling / spring animation (e.g. snapping a card off-screen if released fast enough).

Custom gesture resolution and conflict handling

Situations where ScrollView or other components fight for touch ownership:

For example, horizontally swiping a card inside a vertical list.

Deciding, via onMoveShouldSetPanResponder and onPanResponderTerminationRequest, whether your component or a parent should “own” the gesture.

Multi-touch / advanced gestures (when you really need control)

While it’s more manual than libraries like react-native-gesture-handler, PanResponder can still be used to track multiple touches and build things like pinch-to-zoom or rotate, by inspecting the touches array yourself.

Technically, PanResponder works by registering callbacks such as:

onStartShouldSetPanResponder / onMoveShouldSetPanResponder to decide when to take control of a gesture.

onPanResponderMove to receive continuous gesture updates, including:

gestureState.moveX, moveY – current finger position

gestureState.dx, dy – accumulated deltas from the start

gestureState.vx, vy – velocities, handy for “fling” or “snap” behaviour

onPanResponderRelease / onPanResponderTerminate to finalise the interaction and typically trigger a finishing animation or commit a state change.

In practice, it’s often used together with Animated (or react-native-reanimated) to keep the UI at 60fps while the user drags things around. PanResponder is the raw “gesture signal”; your animation system is the muscles that actually move things.",0.3055555555555556,0.5833333333041666,1.0,0.4438304579319676
How do you perform port scanning and service identification using Nmap in penetration testing?,"['Performing Network Penetration Testing346 As shown in the following screenshot, Nmap was able to identify whether any of the top 1,000 ports were open on the targeted system: Figure 9.22: Checking for open ports 4. Next, let’s identify the operating system and service version of the Metasploitable 2 virtual machine: kali@kali:~$ nmap -A 172.30.1.20 The following screenshot shows that Nmap was able to identify various running services that can be used to research known security vulnerabilities:', 'Network probing with Nmap 137 Service and OS detection There’s blindly knocking on a door, and then there’s reading all the signage out front. Nmap can go well beyond merely establishing the presence of a service – it will have a nice chat with it and gather information about the service. While it runs, Nmap references a database to parse the information and return version information. Y ou can tweak the intensity of the version analysis ( --version-intensity) to a level between 0 and 9. The default is already 7, so you won’t need 8 or 9 until you suspect something esoteric. Similar to a database that helps Nmap parse version information out of conversations with services, Nmap also has a database that contains more than 2,600 OS fingerprints that allow it to determine the host OS based on how the TCP/IP stack behaves. We explored this concept in Chapter 2, Bypassing Network Access Control , when we used p0f to fingerprint OS fingerprints', ""Remote port and service enumeration – nmap 81 MAC Address: E4:E1:30:16:76:C5 (TCT mobile) Service detection performed. Please report any incorrect results at https://nmap.org/submit/ . Nmap done: 256 IP addresses (24 hosts up) scanned in 100.78 seconds Let's try an sV verbose service scan of just 192.168.122.1, port tcp/443 – we'll see that NMAP does a pretty good job of identifying the web server running on that host: root@ubuntu:~# nmap -p 443 -sV 192.168.122.1 Starting Nmap 7.80 ( https://nmap.org ) at 2021-01-06 09:02 PST Nmap scan report for _gateway (192.168.122.1) Host is up (0.0013s latency). PORT STATE SERVICE VERSION 443/tcp open ssl/http nginx MAC Address: 00:0C:29:3B:73:CB (VMware) Service detection performed. Please report any incorrect results at https://nmap.org/submit/"", '. • It identifies the versions of the running services. • It runs a set of vulnerability scanning scripts that come with Nmap. • It attempts to identify the operating system of the target host. The scan command that does all of these things looks like this: sudo nmap -A 192.168.0.37 I guess that you could think of the -A option as the all option since it really does do it all. (Well, almost all, since it doesn’t scan UDP ports.) First, here’s the command that I ran to do the scan: donnie@linux-0ro8:~> sudo nmap -A 192.168.0.37 Here are the results, broken down into sections for formatting purposes: Starting Nmap 6.47 ( http://nmap.org ) at 2017-12-24 19:33 EST Nmap scan report for 192.168.0.37 Host is up (0.00016s latency). Not shown: 996 closed ports Right away, we see that there’s no active firewall on this machine because no ports are in the filtered state. By default, Nmap scans only the 1,000 most popular ports', '. During a penetration test, it’s important to simulate real-world cyberattacks to test the threat detection and mitigation systems within the targeted organization. Avoiding detection with decoys Nmap is usually considered to be the king of network scanners within the cybersecurity industry due to its advanced scanning capabilities like operating system identification, service version detection, and scriptable interactions with the targeted system through the Nmap Scripting Engine (NSE). Nmap enables penetration testers to use decoys when scanning a targeted system. This scanning technique tricks the targeted system into thinking the source of the scan is originating from multiple sources, rather than a single-source IP address that belongs to the attacker machine. To get started with this exercise, please use the following instructions: 1. Power on the Kali Linux, Metasploitable 2, and Metasploitable 3 (Windows version) virtual machines', '. You can specify -p80 to scan for port 80 only on the target and -p- to scan for all 65,535 open ports. As an aspiring ethical hacker and penetration tester, it’s okay if you don’t initially understand the role and function of service ports on a system. However, it is recommended to perform research on anything you’re not familiar with to gain a better understanding of the technology or topic. For instance, there are many service ports and each is associated with a specific application-layer service, such as TCP port 443 being associated with the Hypertext Transfer Protocol Secure (HTTPS) protocol that’s used for secure web communication. By default, Nmap scans TCP ports only. Therefore, if a target is running a service on a User Datagram Protocol (UDP) server port, there’s a possibility you will miss it. To per - form a UDP scan on a port or range of ports, use the -p U:53 command, where 53 is the targeted UDP port number.', "". That wraps up the basics of netcat. We've used netcat to enumerate local ports, connect to and interact with remote ports, stand up some fairly complex local services, and even transfer files. Now let's look at Nmap, a much faster and more elegant method of enumerating remote ports and services. Remote port and service enumeration – nmap The tool that is most widely used to scan network resources is NMAP (short for Network Mapper). NMAP started as a simple port scanner tool, but is well past that set of simple functions now, with a long list of functions. First of all, nmap is not installed by default on a basic Ubuntu workstation (though it is included by default in many other distros). To install it, run sudo apt-get install nmap. As we go forward working with nmap, please try the various commands we're using in our examples. Y ou'll likely see similar results, and will learn about this valuable tool along the way. Y ou may learn lots about your network along the way too!"", 'Network probing with Nmap 141 Figure 5.6 – Identifying our desired timing template Now, we must define the target IP addresses. Thankfully, Nmap gives us the freedom to use more human-friendly methods of defining ranges; a dash between two numbers makes that a range, and you can do it within the octets. So, we know our range starts at 10.10.105.0 and ends at 10.10.115.255. Thus, this makes the specification 10.10.105-115.255. Ah, but wait – we only want the 10 addresses from 10 to 20. Therefore, the specification is 10.10.105-115.10-20. Put it all together to see your command on the screen: Figure 5.7 – The full command, ready for execution Where you will really enjoy Nmap’s power is in Metasploit Console. Let’s take a look. Integrating Nmap with Metasploit Console Suppose you want to run some auxiliary modules in Metasploit and you want to do some host discovery first', '. Identifying all open ports and running services helps penetration testers look for all security vulnerabilities that may exist on the target system. While the list of common syntaxes is a bit too long to cover, you can visit the official Nmap Reference Guide at https://nmap.org/book/man.html for more information.', 'Remote port and service enumeration – nmap 75 Important note One very important caveat on the advice of ""try this out yourself."" NMAP is a pretty innocuous tool, it almost never causes network problems. However, if you are running this against a production network, you will want to get a feel for that network first. There are several classes of gear that have particularly ""rickety"" network stacks – older medical devices for instance, as well as older Industrial Control Systems (ICS) or Supervisory Control and Data Acquisition (SCADA) gear. In other words, if you in are a hospital, a factory, or a utility, take care! Running any network mapping against your production networks can cause issues. Y ou likely do still want to do this, but test against known ""idle"" gear first so you know that when you scan the ""real"" network, you have some assurance that you aren\'t going to cause a problem']","Port scanning and service identification with Nmap is done by running targeted TCP/UDP port scans and using Nmap’s service/OS detection features (for example the -sV and -A options) to enumerate open ports, identify service versions, and attempt OS fingerprinting [6][4][3].

Supporting details
- Basic port selection:
  - Scan specific ports with -p (for example -p80) or scan all TCP ports with -p- (all 65,535 ports) [6].
  - By default Nmap scans the 1,000 most popular TCP ports; UDP must be requested (for example -p U:53 to scan UDP port 53) [4][6].
- Service version detection:
  - Use -sV to perform service/version detection; Nmap queries services and references a version database to parse and return version information [3][2].
  - You can tune the intensity of version probing with --version-intensity (0–9, default 7) [2].
- Aggressive/all-in-one scan:
  - The -A option performs version detection, runs Nmap’s vulnerability scripts, and attempts OS detection (it’s described as doing these together and is commonly used, e.g., sudo nmap -A 192.168.0.37) [4][1][4].
  - Example usage from the documents: nmap -A 172.30.1.20 and sudo nmap -A 192.168.0.37 (these returned service listings and OS/service identification) [1][4].
- Example single-port service scan:
  - nmap -p 443 -sV 192.168.122.1 identified 443/tcp open ssl/http nginx in the example output [3].
- Additional capabilities and cautions:
  - Nmap has an OS fingerprint database (over 2,600 fingerprints) used to determine host OS behaviorally [2].
  - Nmap can use decoys to obfuscate the scan source and supports scriptable interactions via the Nmap Scripting Engine (NSE) for vulnerability checks [5].
  - Be cautious scanning production networks; some devices (e.g., old medical or ICS/SCADA gear) may be fragile and could be affected by scans — test on idle gear first [10].

References
[1] Ultimate Kali Linux Book, Page: 368
[2] Windows And Linux Penetration Testing From Scratch, Page: 159
[3] Linux For Networking Professionals, Page: 103
[4] Mastering Linux Security And Hardening Third Edition, Page: 570
[5] Ultimate Kali Linux Book, Page: 243
[6] Ultimate Kali Linux Book, Page: 239
[7] Linux For Networking Professionals, Page: 96
[8] Windows And Linux Penetration Testing From Scratch, Page: 163
[9] Ultimate Kali Linux Book, Page: 370
[10] Linux For Networking Professionals, Page: 97","After host discovery, Nmap is used to identify open ports, services, and sometimes OS / versions on target systems. A basic scan like:

nmap <target_ip>


scans the top 1,000 TCP ports and reports which are open, closed, or filtered, along with guessed service names (e.g. 80/tcp http, 22/tcp ssh). On something like Metasploitable, this might show 20+ open TCP ports plus their services, which then guide the rest of the security assessment.

You can broaden this to a more “real” penetration-testing flow:

1. Choose scan type (stealth vs simple)

nmap -sS <target> – SYN “stealth” scan (very common in testing).

nmap -sT <target> – TCP connect scan (more noisy but simpler, often default without privileges).

nmap -sU <target> – UDP scan (for things like DNS, SNMP, DHCP; slower and more error-prone).

2. Control which ports to scan

-p 80,443,8080 – specific ports.

-p 1-65535 – full port range (heavier, but thorough).

--top-ports 100 – quickest look at the most common ports.

3. Enable service and version detection
To move beyond “port 80 is open” to “it’s Apache httpd 2.4.x on Ubuntu”, you use:

nmap -sV <target>


This sends probes to open ports and matches responses against Nmap’s service database, giving output such as:

22/tcp open ssh OpenSSH 7.9p1

3306/tcp open mysql MySQL 5.5.5-10.3.25-MariaDB

Combining with ports: nmap -sV -p 1-65535 <target> gives a full service map.

4. Use more in-depth scanning when appropriate

nmap -A <target> – “aggressive” scan:

Enables OS detection (-O)

Version detection (-sV)

Traceroute

Default NSE (Nmap Scripting Engine) scripts

This is handy on lab / test systems, but can be noisy and potentially disruptive on fragile production systems, so in real engagements it’s used selectively and with permission.

5. Enrich results with NSE scripts
NSE scripts let you go beyond basic identification to lightweight enumeration, e.g.:

nmap -sV --script=vuln <target>
nmap -sV --script=http-enum -p80,443 <target>


These can:

Enumerate HTTP directories, SSL info, or SSH host keys.

Check for known, fingerprintable vulnerabilities on certain services.
In a penetration test, this helps quickly spot misconfigurations and low-hanging fruit without going straight to full exploitation.

6. Tuning for performance and safety

-T0…-T5 – timing templates (slow & stealthy vs fast & noisy).

--max-retries, --max-rate – limit how aggressively Nmap probes ports.

Run first simple scans (e.g. nmap -sS --top-ports 100) to map the landscape, then deeper scans only where needed.

7. Exporting and integrating results
For real workflows, you usually export output:

nmap -sV -oN scan.txt -oX scan.xml <target>


Normal output (-oN) for human reading.

XML / JSON output (-oX, -oJ) for feeding into other tools (reporting systems, vulnerability scanners, custom scripts).

In a penetration testing context, Nmap’s role is to systematically map exposed services: which ports are open, what’s running on them, and roughly what versions/OS they are. That service inventory becomes the starting point for everything that follows: threat modelling, deeper enumeration, and eventually targeted testing of those specific services.",0.36363636363636365,0.9206845237980152,0.9166666666666666,0.5785379206268955
What characterizes rack-based servers in warehouse-scale computers?,"['. Rack-mounted servers generally operate in a so-called headless mode, in which all interaction with the system takes place over its network connection. The following diagram shows a rack containing 16 servers:', '. The web server parses the search request and forwards queries to the appropriate index servers in the rack clusters of the WSC. Based on the terms being searched, the web server directs index lookup requests to one or more index servers for processing. To perform efficiently and reliably, the WSC must maintain multiple copies of each subset of the index database, spread across multiple clusters, to provide load sharing and redundancy in case of failures at the server, rack, or cluster level.', "". (The current king-of-the-hill server CPU is the AMD Epyc, which now comes in a 64-core variety that can run 128 simultaneous threads. Y eah, that's enough to make us hard-core geeks salivate.) Although it might seem counter-intuitive, resource management on these beasts is more important than it was on the old systems. That's because one server can now run multiple services, multiple virtual machines, multiple containers, and multiple user accounts all at the same time. A whole roomful of the old physical servers that could only run one or two services can now be replaced by just one physical server. Those simple resource management tools that we used to use still have their uses, but we also now need something a lot more powerful to ensure that all processes and users play nice with each other. Enter cgroups. With cgroups, an administrator can: • Manage resource usage by either processes or users"", ""Chapter 1 [ 5 ] Physical server Nowadays, it's very easy to find used physical servers for an affordable price. Dell PowerEdge is a very common model, and the R610 and R710 specifically are good choices that are readily available. These servers are commonly made available in the reseller market after companies upgrade to newer models. The R610 and R710 are a bit old, but their specs are still great for testing purposes. The downside with physical servers is that they take up a lot of room and can often be power-hungry. Make sure to shut them down when not in use and look into the cost of electric services in your area—these servers can be very cheap to run or very expensive, depending on your electricity rates. Virtual private server Services such as Amazon Web Services, Google Cloud, Linode, Microsoft Azure, Digital Ocean, and others allow you to set up Ubuntu Server in the cloud for you to connect to and manage via OpenSSH""]","Rack-based servers in warehouse-scale computers are headless, network-managed machines mounted in racks and organized into clusters for load sharing and redundancy.

Supporting details
- Rack-mounted servers generally operate in a so-called headless mode, in which all interaction with the system takes place over its network connection [1].  
- WSCs organize servers into rack clusters; web servers forward index queries to appropriate index servers in those rack clusters, and the system maintains multiple copies of each subset of the index database spread across multiple clusters to provide load sharing and redundancy against server, rack, or cluster failures [2].

References
[1] Moderncomputerarchitectureandorganization, Page: 390
[2] Moderncomputerarchitectureandorganization, Page: 392","Rack-based servers in WSCs are typically 1U units (19"" wide, 1.75"" high) mounted in standard 19"" racks, with on the order of a few dozen servers per rack (e.g. ~40 1U servers, depending on power and cooling limits). Each server includes a CPU (often multi-socket or multi-core), RAM, local storage (HDD/SSD), and at least one 1+ Gbit/s Ethernet interface. They usually run headless – no monitor, keyboard or mouse – with all interaction via the network. This form factor aims to maximise compute density and operational efficiency.

You can broaden the description along a few dimensions:

Physical + infrastructure integration

Servers are mounted in racks that also contain top-of-rack (ToR) switches, power distribution units (PDUs), and sometimes rack-level monitoring gear.

Power and cooling constraints (per-rack kW and airflow) often limit how many servers you can actually install, so “up to 40” is a theoretical density; real deployments balance density vs. reliability and cooling.

Racks are arranged in hot/cold aisle layouts to optimise airflow and cooling efficiency at warehouse scale.

Commodity, homogeneous design

Rack servers in WSCs are often commodity or lightly customised hardware, designed to be cheap, consistent, and easy to replace rather than individually “highly reliable”.

Homogeneity (many similar nodes) simplifies automation, scheduling and capacity planning: the system treats each server as a small interchangeable building block.

Networking characteristics

Each server typically connects to a ToR switch via 1 Gbit/s or 10+ Gbit/s Ethernet, with those switches uplinked into a multi-tier fabric (e.g. leaf–spine).

Network design is optimised for east–west traffic (server-to-server inside the WSC) rather than just north–south (client–server), which is critical for large distributed systems.

Management and operations

Servers include out-of-band management (e.g. BMC/IPMI/Redfish), allowing remote power cycling, reimaging, and diagnostics without physical access.

All configuration, software deployment, monitoring, and failure handling is automated through cluster management/orchestration systems; human interaction is at the fleet level, not per machine.

The design assumes frequent hardware failures at scale, so software is built to tolerate node loss; servers are treated as disposable rather than precious.

Serviceability and modularity

Components like disks or PSUs are often hot-swappable, and racks are laid out so technicians can quickly replace failed units in batches.

The balance is between:

high density (more servers per rack),

ease of maintenance (access for human hands), and

staying within power and cooling envelopes.

In short: rack-based servers in WSCs are small, headless, commodity building blocks, tightly integrated with rack-level power, cooling, and networking — optimised less for individual elegance and more for massive-scale, automated, and failure-tolerant operation.",0.058823529411764705,0.9999999999,1.0,0.3238681658348441
What is Roslyn and how does it help developers in .NET?,"['. IDE features like IntelliSense and Find All References need an understanding of the current code base; this is typically information that a compiler can provide. Compilers used to be black boxes that took your source code and transformed it into, in our case, intermediate language. With Roslyn, Microsoft aimed to open up the compiler platform and provide it with an API set for everyone to use to write code enhancing tools. With Roslyn, we can write analyzers and code fixes. Analyzers look at your code and notify you when you write a piece of code that is not according to what the analyzer knows. .NET even ships with a default set of Analyzers; just open a new .NET 6 project and use solution explorer to have a look at Dependencies ➤ Analyzers, as shown in Figure 10-1.', '275 © Nico Vermeir 2022 N. Vermeir, Introducing .NET 6, https://doi.org/10.1007/978-1-4842-7319-7_10 CHAPTER 10 .NET Compiler Platform A part of the strength and flexibility of .NET comes from its compiler platform. Most people have known it under its project name Roslyn. With the compiler platform, developers can analyze their code, enforce coding guidelines, and more. Besides Roslyn, Microsoft has also introduced source generators. Source generators leverage Roslyn to generate code at compile time and include that generated code into the compilation. Roslyn Developers rely heavily on their tools to help their development. Just look at what Visual Studio does to help you write better code, or look at the rich ecosystem of Visual Studio extensions. IDE features like IntelliSense and Find All References need an understanding of the current code base; this is typically information that a compiler can provide', '281 Figure 10-6. Roslyn project templates The Syntax Visualizer is a great visual aid when working with the syntax tree. After installing the .NET Compiler Platform SDK, you will have access to new project templates in Visual Studio. With these templates, you can build your own analyzers, code fixes, or code refactorings, both as a stand-alone console application and as a Visual Studio extension in the VSIX format. These templates are available for both C# and Visual Basic. Chapter 10 .Net Compiler platform', '276 Figure 10-1. Built-in analyzers As you can see, Roslyn can provide numerous checks for best practices. The different icons point to the different severity levels of the checks. Some are warnings; others are errors and will cause builds to fail. Code fixes on the other hand provide proposals to the developer on how to refactor the code to fix an analyzer warning. A code fix can, for example, turn a For Each block into a simple LINQ statement by the push of a button. Figure 10-2 shows an example of this. Chapter 10 .N et Compiler platform', '317 MVVM toolkit, 170–176 project structure, 154, 156 Xamarin forms, 153, 157 N nameof() method, 169 .NET 6 architecture, 21, 22 Blazor, 15–17, 20 CLI, 8, 9, 11 definition, 1 desktop development, 12, 14, 15 desktop packs, 25–29 framework, 1 libraries, 1 managed execution process, 24, 25 MAUI, 17–19 Migration, 119–121 operating systems, 6–8 platform, 4, 5 platform-native APIs, 29 roadmap, 5, 6 supported versions, 3 types, 2 upgrade assistant, 122 version support, 2, 3 Ngen, 25 NuGet packages, 12 NuGet references, 120 O, P, Q OnParametersSet, 139 OpenAPI Initiative (OAI), 204 R Roslyn built-in analyzers, 276 compiler API, 277 compilers, 275 diagnostic API, 278 IDE features, 275 LINQ, 277 scripts, 278 SDK, 279–281 syntax tree, 278 workspace API, 278 Run method, 82 Runtime identifier (RID), 49 Runtimes CoreCLR or Mono, 23 CoreFX, 23 languages, 22 WinRT , 24 S Serverless computing, 245 ServerPrerendered, 147 SetCompatibleTextRenderingDefault method, 82 SignalR, 144 Single-threaded apartment', '. In Chapter 3, we will go into the command line tooling; here we will discover that Visual Studio is not performing any magic tricks, it’s just calling the CLI underneath. In Chapters 4–7, we will learn about the different application frameworks .NET hosts, from native Windows desktop to web applications with ASP .NET Core to cross-platform mobile applications. From there, we cross over into the cloud and see Azure’s support for .NET 6. The final three chapters are a bit more advanced; we go into application architecture and what .NET 6 and C# 10 features help write better architectured code, and we take a look at the compiler platform, or Roslyn. And finally we end on a chapter with some more advanced topics like threading and async/await. The book is written in a demo-based manner. Feel free to pull up your computer and follow along while reading; all the steps are explained so that we can discover the topics together. Happy learning!', '. 270 Installing Dapr ..................................................................................................................... 270 Dapr State Management ......................................................................................................272 Wrapping Up ............................................................................................................................. 273 Chapter 10: .NET Compiler Platform .....................................................................275 Roslyn ....................................................................................................................................... 275 Compiler API ........................................................................................................................ 277 Diagnostic API .....................................................................................................................']","Roslyn is the .NET Compiler Platform — a compiler platform that Microsoft opened up with an API so developers can analyze, inspect, and extend C# and Visual Basic code (for example by writing analyzers, code fixes, refactorings, and source generators) [2], [1].

Supporting details
- Roslyn provides compiler APIs that let tools and IDE features (IntelliSense, Find All References, syntax tree inspection) get an understanding of the current code base — information compilers typically provide — enabling richer IDE support and tooling [1], [2].  
- Developers can write analyzers that look at code and notify when code violates rules; .NET ships with a default set of analyzers that appear under Dependencies → Analyzers in a new .NET 6 project [1], [4].  
- Code fixes propose refactorings to fix analyzer warnings (for example turning a For Each into a LINQ statement) [4].  
- Roslyn also powers source generators, which generate code at compile time and include that generated code in the compilation [2].  
- After installing the .NET Compiler Platform SDK you get project templates in Visual Studio to build analyzers, code fixes, or refactorings (as console apps or VSIX extensions) for both C# and Visual Basic, and tools like the Syntax Visualizer help when working with the syntax tree [3].

References
[1] Introducing Net6, Page: 280
[2] Introducing Net6, Page: 280
[3] Introducing Net6, Page: 286
[4] Introducing Net6, Page: 281","Roslyn is the .NET compiler platform for C# and VB that turns what used to be a black-box compiler into a set of rich, accessible APIs. Instead of “you give it code, it gives you binaries and that’s it”, Roslyn exposes the whole pipeline: parsing, syntax trees, symbol tables, type information, diagnostics, and emitted code.

Building on what you wrote:

It enables developers to write analyzers and code fixes that inspect source code using the same understanding the compiler has. These analyzers can enforce coding standards, detect bugs, flag security issues, and suggest automatic fixes directly in the IDE.

.NET ships with a set of built-in analyzers (style, quality, performance) and you can add your own or those from third parties. They plug into the build, dotnet CLI, and IDEs like Visual Studio / VS Code (via OmniSharp/Roslyn), so the same rules run everywhere.

Roslyn also underpins a lot of the “magic” in modern .NET tooling:

IDE features such as IntelliSense, Find All References, Go To Definition, Rename Symbol, and code refactorings are powered by Roslyn’s syntax trees and semantic model, not ad-hoc string hacks.

It provides workspaces and project/solution APIs so tools can load and understand entire codebases, not just individual files. That’s how refactorings and analyzers stay project-aware (references, preprocessor symbols, multi-targeting, etc.).

Source generators (a Roslyn feature) let you hook into compilation and generate additional C# code at build time based on the existing code or external metadata, avoiding reflection-heavy patterns and boilerplate.

From a developer’s perspective, Roslyn gives you:

A programmable view of code: you can parse, transform, and re-emit C# as easily as you’d manipulate a DOM.

A unified way to implement static analysis, code transformations, and tooling that feel “first-class” in the .NET ecosystem.

The same understanding of code that the compiler uses, which means your tools behave consistently with the language itself.

So Roslyn isn’t just “the C# compiler”: it’s the language engine behind modern .NET development, making analyzers, refactorings, code generation, and rich IDE experiences all hang off one coherent platform.",0.5625,0.8666666666377778,0.8571428571428571,0.5587641157750013
Why should TypeScript developers use noImplicitAny and strictNullChecks?,"["". Once you grow accustomed to all variables having types, TypeScript without noImplicitAny feels almost like a different language. For new projects, you should start with noImplicitAny on, so that you write the types as you write your code. This will help TypeScript spot problems, improve the read‐ ability of your code, and enhance your development experience (see Item 6). Leaving noImplicitAny off is only appropriate if you’re transitioning a project from Java‐ Script to TypeScript (see Chapter 8). strictNullChecks controls whether null and undefined are permissible values in every type. This code is valid when strictNullChecks is off: const x: number = null; // OK, null is a valid number but triggers an error when you turn strictNullChecks on: const x: number = null; // ~ Type 'null' is not assignable to type 'number' A similar error would have occurred had you used undefined instead of null"", ""const x: number | null = null; If you do not wish to permit null, you’ll need to track down where it came from and add either a check or an assertion: const el = document.getElementById('status'); el.textContent = 'Ready'; // ~~ Object is possibly 'null' if (el) { el.textContent = 'Ready'; // OK, null has been excluded } el!.textContent = 'Ready'; // OK, we've asserted that el is non-null strictNullChecks is tremendously helpful for catching errors involving null and undefined values, but it does increase the difficulty of using the language. If you’re starting a new project, try setting strictNullChecks. But if you’re new to the lan‐ guage or migrating a JavaScript codebase, you may elect to leave it off. Y ou should certainly set noImplicitAny before you set strictNullChecks. If you choose to work without strictNullChecks, keep an eye out for the dreaded “undefined is not an object” runtime error. Every one of these is a reminder that you should consider enabling stricter checking"", '. • Configure TypeScript using tsconfig.json rather than command-line options. • Turn on noImplicitAny unless you are transitioning a JavaScript project to TypeScript. • Use strictNullChecks to prevent “undefined is not an object”-style runtime errors. Item 2: Know Which TypeScript Options You’re Using | 9', '. When your types match reality, it frees you from the burden of having to keep type information in your head. TypeScript will keep track of it for you. For the times when you must use any, there are better and worse ways to do it. For much more on how to limit the downsides of any, see Chapter 5. Things to Remember • The any type effectively silences the type checker and TypeScript language serv‐ ices. It can mask real problems, harm developer experience, and undermine con‐ fidence in the type system. Avoid using it when you can! Item 5: Limit Use of the any Type | 23', '// ... } } A good strategy for enabling noImplicitAny is to set it in your local client and start fixing errors. The number of errors you get from the type checker gives you a good sense of your progress. Y ou can commit the type corrections without committing the tsconfig.json change until you get the number of errors down to zero. There are many other knobs you can turn to increase the strictness of type checking, culminating with ""strict"": true. But noImplicitAny is the most important one and your project will get most of the benefits of TypeScript even if you don’t adopt other settings like strictNullChecks. Give everyone on your team a chance to get used to TypeScript before you adopt stricter settings. Things to Remember • Don’t consider your TypeScript migration done until you adopt noImplicitAny. Loose type checking can mask real mistakes in type declarations. • Fix type errors gradually before enforcing noImplicitAny', '. } TypeScript is generally quite good at tracking types through conditionals. Think twice before adding an assertion—it might be onto something that you’re not! For example, this is the wrong way to exclude null from a union type: const el = document.getElementById(\'foo\'); // type is HTMLElement | null if (typeof el === \'object\') { el; // Type is HTMLElement | null } Because typeof null is ""object"" in JavaScript, you have not, in fact, excluded null with this check! Similar surprises can come from falsy primitive values: function foo(x?: number|string|null) { if (!x) { x; // Type is string | number | null | undefined } } 94 | Chapter 3: Type Inference', '. Loose type checking can mask real mistakes in type declarations. • Fix type errors gradually before enforcing noImplicitAny. Give your team a chance to get comfortable with TypeScript before adopting stricter checks. 234 | Chapter 8: Migrating to TypeScript', "". Y ou can think of it as an extremely loose version of type checking: looser even than TypeScript with noImplicitAny off (Item 2). Here’s how it works: // @ts-check const person = {first: 'Grace', last: 'Hopper'}; 2 * person.first // ~~~~~~~~~~~~ The right-hand side of an arithmetic operation must be of type // 'any', 'number', 'bigint', or an enum type TypeScript infers the type of person.first as string, so 2 * person.first is a type error, no type annotations required. While it may surface this sort of blatant type error, or functions called with too many arguments, in practice, // @ts-check tends to turn up a few specific types of errors: 222 | Chapter 8: Migrating to TypeScript"", '. If you mean to allow null, you can fix the error by making your intent explicit: 8 | Chapter 1: Getting to Know TypeScript']","TypeScript developers should enable noImplicitAny to avoid silent any types that mask real problems and harm developer experience, and enable strictNullChecks to prevent null/undefined from being treated as valid values for every type (thus catching “undefined is not an object” runtime errors) [1][3][4][2].

Supporting details
- noImplicitAny:
  - When off, variables can implicitly be any, which “effectively silences the type checker and TypeScript language services,” can mask real problems, harm developer experience, and undermine confidence in the type system; turning on noImplicitAny forces you to write types as you write code and helps TypeScript spot problems and improve readability [1][4][5].
  - It is recommended to set noImplicitAny before strictNullChecks and to enable it for new projects; during migration from JavaScript you may leave it off temporarily, fixing errors gradually before enforcing it [1][5][2].

- strictNullChecks:
  - Controls whether null and undefined are permissible values in every type. With strictNullChecks off, code like const x: number = null is allowed; with it on, that assignment is an error (Type 'null' is not assignable to type 'number') [1].
  - If you intend to allow null, you must make it explicit (e.g., number | null) or add runtime checks or non-null assertions; this helps catch null/undefined-related bugs but can increase typing effort [2].
  - Using strictNullChecks helps prevent runtime “undefined is not an object” errors; the recommendation is to try it for new projects but consider leaving it off while first migrating a JS codebase [2][3].

Finally, practical advice from the documents:
- Configure TypeScript via tsconfig.json, turn on noImplicitAny for new projects, and use strictNullChecks to reduce null/undefined runtime errors; adopt noImplicitAny first and gradually enable stricter checks as your team becomes comfortable [3][5][2].

References
[1] Effective Typescript, Page: 27
[2] Effective Typescript, Page: 28
[3] Effective Typescript, Page: 28
[4] Effective Typescript, Page: 42
[5] Effective Typescript, Page: 253","noImplicitAny and strictNullChecks turn TypeScript from “JavaScript with hints” into a proper safety net.

noImplicitAny

Forces every variable, parameter, and return value to have a known type, instead of silently falling back to any.

This means the compiler can actually reason about your code: it will catch misspelled properties, wrong argument shapes, and incorrect return types that would otherwise slip through.

It also improves readability and maintainability: when you revisit code in six months, you can see exactly what’s expected, rather than guessing from usage.

If you really do need escape hatches, you can still use any – but now you do it deliberately, and reviewers can spot and question it.

strictNullChecks

Treats null and undefined as proper inhabitants of their own types instead of being allowed everywhere.

A variable of type number can’t silently become undefined; you must write number | undefined (or similar) and handle that case.

This catches a whole class of “cannot read property ‘x’ of undefined” / “undefined is not an object” bugs at compile time, by forcing you to deal with potentially missing values via checks, optional chaining, default values, or narrowing.

It also pushes you towards clearer domain modelling – e.g. “this field is optional” vs “this is always present once initialised”.

In practice:

New projects should almost always enable the top-level ""strict"": true option, which includes both noImplicitAny and strictNullChecks.

Existing/legacy codebases often move there gradually: switch on noImplicitAny, clean things up, then enable strictNullChecks and tackle nullability in key modules first.

Although they can make the first few days slightly more work, they dramatically reduce runtime bugs, make refactoring safer, and give tooling (IntelliSense, refactors) much better information to work with.

Short version: turning these on stops TypeScript from politely looking away while you write JavaScript, and makes it actually guard your code.",0.9230769230769231,0.94999999997625,1.0,0.5912420398430136
What are the main components of AI orchestrators in LLM-powered applications?,"['. Ideally, it should notify you when this data flow is disrup‐ ted due to errors such as component failures or data mismatch failures. An AI pipeline orchestrator is different from a general workflow orchestrator, like Airflow or Metaflow. When designing the pipeline for an application with strict latency requirements, try to do as much in parallel as possible. For example, if you have a routing component (deciding where to send a query) and a PII removal component, both can be done at the same time. There are many AI orchestration tools, including LangChain, LlamaIndex, Flowise, Langflow, and Haystack. Because retrieval and tool use are common application pat‐ terns, many RAG and agent frameworks are also orchestration tools. While it’s tempting to jump straight to an orchestration tool when starting a project, you might want to start building your application without one first. Any external tool brings additional complexity', '. • Have an LLM decide what steps are available to take (for instance, have the• LLM write code that executes a dynamic action you didn’t preprogram into the application). We can classify a number of popular recipes for building LLM applications based on where they fall in this spectrum of autonomy, that is, which of the three tasks just mentioned are handled by an LLM and which remain in the hands of the developer or user. These recipes can be called cognitive architectures. In the artificial intelligence field, the term cognitive architecture has long been used to denote models of human reasoning (and their implementations in computers). An LLM cognitive architecture (the term was first applied to LLMs, to our knowledge, in a paper 1) can be defined as a recipe for the steps to be taken by an LLM application (see Figure 5-1). A step is, for instance, retrieval of relevant documents (RAG), or calling an LLM with a chain-of-thought prompt', '. • Advanced reasoning algorithms enable complex, multi-step inference chains. • Dynamic responses based on the LLM or on up-to-date information retrieved in real time. The key capability LLM apps use is the ability to understand nuanced language in prompts and generate coherent, human-like text responses. This facilitates more natural interactions and workflows compared to traditional code. The LLM provides human-like language capabilities without manual coding. Therefore, there is no need to manually anticipate and code every language scenario in advance. The integration of LLMs with external services, knowledge, and reasoning algorithms eases the development of innovative applications. But responsible data practices are critical – PII should be kept off public platforms and models should be fine-tuned in-house when needed. Both the frontend and the output parser could include moderation and enforcing rules about behavior, privacy, and security', 'LLMs for AI-Powered Applications40 Summary In this chapter, we delved into the new way of developing applications that LLMs have been paving, as we introduced the concept of the copilot and discussed the emergence of new AI orchestrators. Among those, we focused on three projects – LangChain, Haystack, and Semantic Kernel – and we examined their features, main components, and some criteria to decide which one to pick. Once we have decided on the AI orchestrator, another pivotal step is to decide which LLM(s) we want to embed into our applications. In Chapter 3, Choosing an LLM for Your Application, we are going to see the most prominent LLMs on the market today – both proprietary and open-source – and understand some decision criteria to pick the proper models with respect to the application use cases', '. You may want to choose a framework that has the availability and quality of documentation, tutorials, examples, and community support that can help you get started and solve problems with the framework. Let’s briefly summarize the differences between these orchestrators: Feature LangChain Haystack Semantic Kernel LLM support Proprietary and open-source Proprietary and open source Proprietary and open source Supported languages Python and JS/TS Python C#, Java, and Python Process orchestration Chains Pipelines of nodes Pipelines of functions Deployment No REST API REST API No REST API Feature LangChain Haystack Semantic Kernel Table 2.1: Comparisons among the three AI orchestrators Overall, all three frameworks offer a wide range of tools and integrations to build your LLM-powered applications, and a wise approach could be to use the one that is most in line with your current skills or the company’s overall approach.', '. The main components of AI orchestrators From one side, the paradigm shift of foundation models implies a great simplification in the domain of AI-powered applications: after producing models, now the trend is consuming models. On the other side, many roadblocks might arise in developing this new kind of AI, since there are LLM-related com- ponents that are brand new and have never been managed before within an application life cycle. For example, there might be malicious actors that could try to change the LLM instructions (the system message mentioned earlier) so that the application does not follow the correct instructions. This is an example of a new set of security threats that are typical to LLM-powered applications and need to be addressed with powerful counterattacks or preventive techniques. Definition Prompt engineering is the process of designing and optimizing prompts to LLMs for a wide variety of applications and research topics', 'Developing and Operationalizing LLM-based Apps: Exploring Dev Frameworks and LLMOps142 AutoGen Autogen, an agent collaboration framework introduced by Microsoft Research, is another major breakthrough in AI. It is an open source platform for building multi-agent systems that work autonomously using LLMs, and we feel this will have one of the most significant impacts in the generative AI space in the upcoming months and years (https://arxiv.org/abs/2308.08155). AutoGen can help build agents that perform tasks such as reasoning, planning, task decomposition, reflection, self-critique, self-improvement, self-evaluation, memory, personalization, and communication by using various prompt engineering techniques, just to name a few areas. Of course, as mentioned above, autonomous agents can call on other autonomous agents to help address the most complex of problems or situations', '. Therefore, the rise of the transformer architecture has been a tide that lifted all boats, revolutionizing not just one but multiple fields within AI. However, this versatility also brings unique security challenges across these various applications. As we look more deeply into LLM security, we’ll explore how the ubiquitous nature of trans‐ former architectures necessitates a multifaceted approach to safeguarding AI systems. Types of LLM-Based Applications Two common types of LLM-based applications are chatbots and copilots. Let’s briefly look at each to help you understand the breadth of applications in which developers use LLMs and give you context for understanding various architectural choices as you study further. 16 | Chapter 3: Architectures and Trust Boundaries', 'Our questions and the experts’ answers 295 Finally, we can combine LLMs at the application layer by treating each LLM instance as an agent. This leads to the notion of multi-agent systems where LLM-powered agents that are specialized for a task are combined to do a more complex one. Q2 – AI-driven organizational structure – in what ways do you predict AI will continue to reshape internal business operations, and how should companies prepare to adapt their organizational structures to maximize the benefits of AI, especially in decision- making and operational efficiency? Generative AI is going to revolutionize every aspect of organizations. My strong prediction is that AI is going to become another member of the organization. For example, software engineers will collaborate with an AI (or several of them) in their day to day. This will make them not 10X but 100X more efficient']","The main components of AI orchestrators in LLM-powered applications are the LLM-related building blocks (prompts and prompt engineering), orchestration primitives (chains/pipelines/nodes/functions), integrations for retrieval and tools, process orchestration and deployment features, and security/privacy controls (e.g., protecting system messages and PII) [6], [5], [1], [3].

Supporting details
- Prompt engineering and LLM instructions: designing and optimizing prompts and protecting LLM instructions (system message) are core parts of orchestrators because they control model behavior and introduce new security threats unique to LLM apps [6].  
- Orchestration primitives: orchestrators implement process orchestration via constructs such as chains (LangChain), pipelines of nodes (Haystack), or pipelines of functions (Semantic Kernel) to sequence or parallelize steps [5].  
- Retrieval, tool use, and agent patterns: retrieval-augmented generation (RAG), tool calling, and agent frameworks are common orchestration patterns and often built into these tools [1], [2].  
- Reasoning and dynamic responses: advanced reasoning algorithms, multi-step inference, and dynamic responses using up-to-date retrieved information are capabilities supported by orchestrators [3].  
- Deployment and integrations: orchestrators differ in deployment features (e.g., REST API presence) and language/runtime support; they provide integrations with LLMs and external services [5].  
- Monitoring, parallelism, and error handling: orchestrators should support parallel execution for latency-sensitive pipelines, notify on disrupted data flows (component failures or data mismatch), and handle errors across components [1].  
- Privacy and security controls: components for PII removal, moderation, and rules enforcement (frontend/output parser) are part of orchestrator concerns to ensure privacy and safe behavior [3], [1].

References
[1] Ai Engineering, Page: 496  
[2] Learning LangChain, Page: 143  
[3] Generative AI With LangChain, Page: 67  
[5] Building Llm Powered Applications, Page: 59  
[6] Building Llm Powered Applications, Page: 51","AI orchestrators (like LangChain, Semantic Kernel, Haystack, etc.) are lightweight frameworks that sit between your app and one or more LLMs. They don’t just “call the model”; they structure how calls happen, in what order, with what context, and under which rules.

Building on your summary, you can frame their main components roughly like this:

Model + prompt management layer

Abstractions over different LLM providers (OpenAI, Azure, local models, etc.).

Prompt templates and prompt pipelines for:

Frontend prompts: user-facing queries.

Backend/meta prompts: system instructions, planning prompts, self-critique, tool-selection prompts, etc.

Support for multi-model routing (e.g. cheap model for classification, bigger model for reasoning).

Chaining / workflow / graph execution

Primitives like chains, pipelines, or graphs to connect multiple LLM calls, tools, and retrieval steps.

Control flow: branching, looping, error handling, timeouts, retries.

“Agent” patterns, where an LLM repeatedly plans → calls tools → reflects → continues until a goal is reached.

Context and retrieval components

Connectors to vector databases / search backends (Pinecone, Qdrant, Elasticsearch, etc.).

Retrievers that take a user query and fetch relevant docs/chunks.

Mechanisms to build and maintain embeddings indexes from documentation, knowledge bases, and user data.

Automatic construction of the final context window (chunk selection, re-ranking, trimming).

Memory and state management

Short-term memory: conversation history, intermediate reasoning steps.

Long-term memory: user profiles, preferences, previous tasks.

Structured state objects passed between steps in a chain/graph.

Security, safety, and policy enforcement

Input/output filters for prompt injection, data leakage, jailbreak attempts, PII, etc.

Policy layers that enforce constraints (e.g. which tools a given user/tenant can invoke).

Guards around external tool calls (rate limits, allowed domains, parameter validation).

Tooling and integration layer

Wrappers around tools/APIs: databases, search engines, code execution, business systems.

Function/tool-calling integration so LLMs can decide which tool to call and with what arguments.

Connectors to queues, event systems, HTTP endpoints, cron jobs, etc., so LLM workflows can live inside real apps.

Observability, evaluation, and optimisation

Logging and tracing of every step (prompts, responses, latency, errors).

Hooks for evaluation: automatic tests, human rating loops, regression suites.

Analytics for cost, token usage, and model comparison.

Sometimes built-in support for A/B testing prompt variants or different workflows.

Runtime / deployment concerns

Configuration-driven setup (YAML/JSON) for models, prompts, tools and routes.

Support for multi-tenant environments, environments (dev/stage/prod), and secrets management.

Caching of LLM results to reduce cost and latency.

So instead of just “a library that calls an LLM”, an orchestrator is more like middleware for AI workflows: it manages prompts at multiple levels, coordinates retrieval and tools, enforces security and constraints, and gives you the monitoring and structure needed to run LLM-powered features reliably in production.",0.375,0.0,0.6538461538461539,0.5971812100738847
Explain activation functions in neural networks and their purpose.,"['Black-Box Methods – Neural Networks and Support Vector Machines272 The primary detail that differentiates these activation functions is the output signal range. Typi- cally, this is one of (0, 1), (-1, +1), or (-inf, +inf). The choice of activation function biases the neural network such that it may fit certain types of data more appropriately, allowing the construction of specialized neural networks. For instance, a linear activation function results in a neural net - work very similar to a linear regression model, while a Gaussian activation function is the basis of a radial basis function (RBF) network. Each of these has strengths better suited for certain learning tasks and not others. It’s important to recognize that for many of the activation functions, the range of input values that affect the output signal is relatively narrow. For example, in the case of the sigmoid, the output signal is very near 0 for an input signal below -5 and very near 1 for an input signal above 5', '. Activation functions Activation functions are non-linear differentiable functions. In a biological neuron, the axon hillock decides whether to fire a signal based on the inputs. The activation functions serve a similar function and are key to the neural network’s ability to model non-linear data. In other words, activation functions are key in neural networks’ ability to transform input vector space (which is linearly inseparable) to a linearly separable vector space, informally. To unwarp a space such that linearly inseparable points become linearly separable, we need to have non-linear transformations. We repeated the same experiment we did in the last section, where we visualized the trained trans- formation of a neural network on the input vector space, but this time, without any non-linearities. The resulting video can be found here: https://www.youtube.com/watch?v=z-nV8oBpH2w', '. Activation functions are functions applied to the features in hidden layers before passing along those values to the next layer. This is done so that the functions that go from one layer to the next are no longer linear, and the model can learn more interesting behavior. The most common activation function used for neural networks (and the simplest) is the rectified linear unit function, or ReLU for short. ReLU is a very simple function that takes in a single value and returns that value if it’s positive, or returns 0 if it’s negative (see Figure 6-30). Neural Networks in BigQuery ML | 185', '27Neural network basics 2.1.8 Activation functions When training or predicting (via inference), each node in a layer will output a value to the nodes in the next layer. We don’t want to pass the value as-is, but instead sometimes want to change the value in a particular manner. This process is called an activation function. Think of a function that returns a result, like return result. In the case of an acti- vation function, instead of returning result, we would return the result of passing the result value to another (activation) function, like return A(result), where A() is the activation function. Conceptually, you can think of this as follows: def layer(params): """""" inside are the nodes """""" result = some_calculations return A(result) def A(result): """""" modifies the result """""" return some_modified_value_of_result Activation functions assist neural networks in learning faster and better', 'Figure A-8. Different transfer or activation functions With these tools we are ready to introduce neural networks. A neural network is a set of nodes and edges or connections between the nodes like the one depicted in Figure A-9 . The left panel shows a network with only two inputs, each of which affects two hidden or intermediate nodes, the strength mediated by the correspond‐ ing parameters. We may decide to use nonlinear activation functions in each of the intermediate nodes. Finally, once we have the strength of each hidden unit we are ready to aggregate the impact onto the output, mediated again by corresponding weights and possibly an activation function. It may not be immediately obvious, but these arrangements of nodes and edges were originally designed to emulate how our brains work, nodes being the neurons that are connected through synapses or edges in the network. These days, few practitioners take this analogy literally', '. The activation function is critical to ensure the neural network is able to learn complex functions and doesn’t just output a linear combination of its inputs. 32 | Chapter 2: Deep Learning', '. Common choices for activation functions include the sigmoid function, hyperbolic tangent (tanh), and rectified linear unit (ReLU), among others. The output of the neuron is the result of the activation function. It serves as the input to the neurons in the next layer of the network. The weights and bias in the neuron are learnable parameters. In other words, their values are learned over time as the neural network is trained on data: \x82 Weights: The strength or amplitude of the connection between two neurons. During the training phase, the neural network learns the correct weights that better map inputs to outputs. Weight is used in the neuron, as explained previously. \x82 Bias: An additional parameter in the neuron that allows for the activation function to be shifted to the left or right, which can be critical for successful learning (also used in the neuron)', '. By default, when no activation function is specified, the values from one layer are passed as-is (unchanged) to the next layer. The mo st basic activation function is a step function. If the value is greater than 0, a 1 is outputted; otherwise, a 0 is outputted. The step func- tion hasn’t been used in a long, long time. Let’s pause for a moment and discuss the purpose of an activation function. You likely have heard the term nonlinearity. What is this? To me, more importantly, what is it not? In traditional statistics, we worked in low-dimensional space with a strong linear correlation between the input and output. Th is correlation could be computed as a polynomial transformation of the input that, when transf ormed, had a linear correla- tion to the output. The most fundamental example is the slope of a line, which is rep- resented as y = mx + b', '. Activation functions The activation function is the mechanism by which the artificial neuron processes incoming information and determines whether to pass the signal to other neurons in the network. Just as an artificial neuron is modeled after the biological version, so too is the activation function modeled after nature’s design. In the biological case, the activation function could be imagined as a process that involves sum- ming the total input signal and determining whether it meets the firing threshold. If so, the neu- ron passes on the signal; otherwise, it does nothing. In ANN terms, this is known as a threshold activation function, as it results in an output signal only once a specified input threshold has been attained. The following figure depicts a typical threshold function; in this case, the neuron fires when the sum of input signals is at least zero. Because its shape resembles a stair, it is sometimes called a unit step activation function.', '. Please note that the activation method has no effect in the code since it is simply an identity function. Here, we added the activation function (computed via the activation method) to illustrate the general concept with regard to how information flows through a single-layer NN: features from the input data, net input, activation, and output.']","Activation functions are functions applied to a neuron's weighted input (net input) that transform that value before passing it to the next layer; their purpose is to introduce nonlinearity so the network can learn and represent complex, non‑linear functions rather than just linear combinations of inputs [9][3][6].

Supporting details
- Purpose and role:
  - Activation functions decide whether and how a neuron “fires” by processing incoming information and determining whether to pass a signal to other neurons, analogous to a biological neuron’s threshold behavior [9].  
  - They introduce nonlinearity that lets networks transform an input vector space that is linearly inseparable into a space that can be linearly separated, enabling the model to learn complex patterns [2][6][4].  
  - Without nonlinearity, a network would collapse to a linear model (e.g., similar to linear regression) and cannot learn richer behaviors [1][6].
- Common types and characteristics:
  - Step (unit step / threshold) function: outputs 1 if input > 0, otherwise 0; historically used but rarely used today [8][9].  
  - Sigmoid and hyperbolic tangent (tanh): produce bounded outputs (typical ranges include (0,1) for sigmoid and (-1,+1) for tanh); many activation functions have narrow input ranges that strongly affect outputs (e.g., sigmoid outputs very near 0 for inputs below -5 and very near 1 for inputs above 5) [1][7].  
  - Rectified Linear Unit (ReLU): returns the input if positive, else 0; commonly used because of simplicity and effectiveness [3][7].  
  - Linear and Gaussian activations: a linear activation yields a network similar to linear regression, while a Gaussian activation underlies radial basis function (RBF) networks—different activations bias networks toward different problem types [1].
- Practical notes:
  - By default (no activation) a layer passes values unchanged (identity) and thus no nonlinearity is added [10].  
  - Activation functions are applied to hidden-layer outputs before passing values to the next layer, helping the model “learn more interesting behavior” and improving training speed and quality [3][4].

References
[1] Machinelearningwithr Fourthedition, Page: 294  
[2] Moderntimeseriesforecastingwithpython Secondedition, Page: 316  
[3] Low Codeai V4, Page: 200  
[4] Deeplearningpatternsandpractices, Page: 50  
[6] Generativedeeplearning Secondedition V4, Page: 59  
[7] Masteringnlpfromfoundationstollms, Page: 167  
[8] Deeplearningpatternsandpractices, Page: 50  
[9] Machinelearningwithr Fourthedition, Page: 291  
[10] Pythonmachinelearning Thirdedition, Page: 69","Activation functions are the parts of a neural network that decide **how much a neuron “fires” given its input**, and they’re crucial because they make the whole thing more than just a fancy linear regression.

Here’s a tighter but broader explanation:

* **Core purpose: add nonlinearity**
  Without activation functions, every layer of a feed-forward network would just combine linearly with the others, and the whole network would collapse to a single linear transformation. You’d never be able to model non-linear patterns like XOR, complex decision boundaries, or most interesting real-world relationships.
  Activation functions break that linearity so deep networks can approximate highly complex functions.

* **What makes a “good” activation function?**
  We usually want them to be:

  * **Non-linear** – otherwise depth is pointless.
  * **Differentiable (or nearly)** – so gradient-based optimisation (backprop) works.
  * **Well-behaved gradients** – not all squashed to zero (vanishing) or exploding.
  * Often **cheap to compute**, because you’re doing this millions or billions of times.

* **Common activation functions and where they show up**

  * **ReLU (Rectified Linear Unit)**:
    [
    f(x) = \max(0, x)
    ]
    Zero for negative inputs, identity for positive ones. It’s simple, fast, and avoids vanishing gradients for positive values, which is why it became the default in deep nets. Downsides: “dead ReLUs” (neurons stuck at zero if weights push them permanently negative).

  * **Leaky ReLU / variants (PReLU, ELU, GELU, etc.)**
    These are tweaks to ReLU that let a small gradient flow for negative values (e.g. `max(αx, x)`), or smooth the shape, to reduce dead neurons and sometimes improve performance and training stability.

  * **Sigmoid**:
    [
    \sigma(x) = \frac{1}{1 + e^{-x}}
    ]
    Smoothly squashes inputs to (0,1). Historically used everywhere; now mainly used in **output layers** for binary classification or gates in LSTMs/GRUs. In hidden layers it tends to cause **vanishing gradients** because the function saturates near 0 and 1.

  * **tanh**:
    Like a rescaled sigmoid, outputs in (−1,1). Zero-centred, which usually trains better than sigmoid in hidden layers, but still suffers from saturation/vanishing gradients in deep networks.

  * **Softmax**:
    Used in **output layers** for multi-class classification. Turns a vector of scores into a probability distribution (values in (0,1) that sum to 1). It’s not used per-neuron in hidden layers, but still an activation function conceptually.

* **Different roles in hidden vs output layers**

  * **Hidden layers** use ReLU-style or tanh/sigmoid-style activations to build up rich internal representations.
  * **Output layers** choose activations to match the task:

    * Regression: often **linear** (no activation).
    * Binary classification: **sigmoid**.
    * Multi-class classification: **softmax**.

* **Why they matter beyond “nonlinearity”**

  Activation functions affect:

  * **Trainability**: ease of optimisation, susceptibility to vanishing/exploding gradients.
  * **Speed**: both training and inference cost.
  * **Generalisation**: some choices empirically give better performance on real data.

So yes, they started as a rough analogy to biological neuron firing, but in modern practice they’re chosen as **engineering components**: mathematical shapes that make deep networks trainable, expressive, and reasonably well-behaved, so the model can actually learn the messy non-linear structure of real data.",0.5,0.9111111110928889,1.0,0.7122638858876196
How does Bayesian Optimization work for black box function optimization?,"['. Bayesian search balances exploration with exploitation. At the start, it explores randomly, thus training the surrogate function as it goes. Based on that surrogate function, the search exploits its initial approximate knowledge of how the predictor works in order to sample more useful examples and minimize the cost function. As the Bayesian part of the name suggests, we are using priors in order to make smarter decisions about sampling during optimization. This way, we reach a minimization more quickly by limiting the number of evaluations we need to make. Bayesian optimization uses an acquisition function to tell us how promising an observation will be. In fact, to manage the tradeoff between exploration and exploitation, the algorithm defines an acquisition function that provides a single measure of how useful it would be to try any given point. Usually, Bayesian optimization is powered by Gaussian processes', '1 Introduction to Bayesian optimization You’ve made a wonderful choice in reading this book, and I’m excited for your upcoming journey! On a high level, Bayesian optimization is an optimization tech- nique that may be applied when the function (or, in general, any process that gen- erates an output when an input is passed in) one is trying to optimize is a black box and expensive to evaluate in terms of time , money, or other re sources. This setup encompasses many important tasks, incl uding hyperparameter tuning, which we define shortly. Using Bayesi an optimization can accelera te this search procedure and help us locate the optimum of the function as quickly as possible. While Bayesian optimization has enjo yed enduring interest from the machine learning (ML) research community, it’s not as commonly used or talked about as other ML topics in practice', '. Throughout the book, we run our algorithms on the same dataset in many different settings so we can compare and analyze the different approaches taken. Summary \uf0a1 Many problems in the real world may be cast as expensive black box optimiza- tion problems. In these problems, we on ly observe the function values without any additional information. Further, observing one function value is expensive, rendering many cost-blind optimization algorithms unusable. \uf0a1 BayesOpt is an ML technique that solv es this black box optimization problem by designing intelligent evaluations of the objective function so the optimum may be found as quickly as possible. \uf0a1 In BayesOpt, a GP acts as a predictive model, predicting what the value of the objective function is at a given locati on. A GP produces no t only a mean pre- diction but also a 95% CI, representing uncertainty via normal distributions', '. To choose which hyperparameters to explore next, one can optimize the expected improvement (E I) over the current best result or the Gaussian process upper confidence bound (UCB). EI and UCB have been shown to be efficient in the number of function ev aluations required to find the global opti- mum of multimodal black-box functions. Bayesian optimization uses all the inform ation available from previous evaluations of the objective function, as opposed to relying on the local gradient and Hessian approximations. This results in an automated procedure that can find an optimum of non-convex functions with relatively few ev aluations, at the cost of performing more computation to determine the next point to try. This is particularly useful when evalu- ations are expensive to perform, such as in selecting hyperparameters for deep neural networks. The Bayesian optimization algorithm is summarized in figure 7.12', '. The central question in expensive black box optimization is this: What is a good way to decide where to evaluate this objective function so its optimum may be found at the end of the search? As we see in a later ex ample, simple heuristics—such as random or grid search, which are approaches implemented by popular Python packages like scikit- learn—may lead to wasteful evaluations of the objective function and, thus, overall poor optimization performance. This is where Bayesian optimization comes into play. 1.2 Introducing Bayesian optimization With the problem of expensive black box optimization in mind, we now introduce Bayesian optimization as a solution to this problem. This gives us a high-level idea of what Bayesian optimization is and how it us es probabilistic ML to optimize expensive black box functions', '. Running Bayesian hyperparameter tuning Bayesian optimization is a global optimization method that uses the posterior distribution of black-box objective functions and their continuous parameters. In other words, it sequentially searches the best parameters to test next based on past results. Unlike grid search, it doesn’t try fixed combinations of parameters on a grid but exploits what it already knows and explores the unknown. The bayesian-optimization library is model-agnostic. All it needs is a function and parameters with their bounds. It will explore values for those parameters within those bounds. The function takes those parameters and returns a number. This is the number, or target, that the Bayesian optimization algorithm will maximize. The following code is for the objective function, which initializes a RepeatedStratifiedKFold cross-validation with four splits and three repeats. It then iterates across the splits and fits the CatBoostClassifier with them', '. 1.2.4 BayesOpt in action At this point, you might be wondering wh ether all of this heavy machinery really works—or works better than some simple strategy like random sampling. To find out, let’s take a look at a “demo” of BayesOpt on a simple function. This will also be a good way for us to move away from the abstract to the concrete and tease out what we are able to do in future chapters. Let’s say the black box objective function we are trying to optimize (specifically, in this case, maximize) is the one-dimensional function in figure 1.6, defined from –5 to 5. Again, this picture is only for our reference; in black box optimization, we, in fact, do not know the shape of the objective. We see the objective has a couple of local maxima around –5 (roughly, –2.4 and 1.5) but the global maximum is on the right at approximately 4.3. Let’s also assume we are allowed to evaluate the objective function a maximum of 10 times', '25510.1 Black-box optimization with pairwise comparisons first by using a GP model that can effect ively learn from pairwise comparisons. We then develop strategies that pit the best data point (representing a product) we have found so far against a promising rival, allowing us to optimize the user’s prefer- ence as quickly as possible. In other words, we assume the user’s preference is the objective function defined over a search spac e, and we’d like to optimize this objec- tive function. This setting of learning and optimizing a user’s preference from pairwise compari- sons is a unique task that lies at the inte rsection of black box optimization and prod- uct recommendation and has been gaining in terest in both communities. By the end of this chapter, we learn how to approach this problem from the BayesOpt, trading off exploitation and exploration as we collect data from the user', '. Black-box optimization methods are the general approach to the optimization problem, when you treat the objective that you’re optimizing as a black box, without any assumptions about the differentiability, the value function, the smoothness of the objective, and so on. The only requirement that those methods expose is the ability to calculate the fitness function, which should give us the measure of suitability of a particular instance of the optimized entity at hand.', '. Recall our ultimate goal in using BayesOpt: to search within the domain D of a black box function at the location where the function is maximized. This means a Justifying binary search with information theory When I first learned about this algorithm, I remember thinking that the strategy of searching in the middle of an array seemed, although reasonable, quite unique and “out of nowhere.” However, we have just learned to derive the same solution from an information-theoretic perspective, which concretely quantifies the idea of ruling out half of our search space in the service of gaining as much information as possible or reducing as much entropy as possible.']","Bayesian optimization is a sequential method for optimizing expensive black‑box functions that builds a probabilistic surrogate model (usually a Gaussian process) of the objective, uses that model to quantify uncertainty, and selects new evaluation points by optimizing an acquisition function that trades off exploration and exploitation [1][3][4].

Supporting details
- Purpose and setup: It is designed for black‑box problems where individual function evaluations are expensive (time, money, etc.), and we only observe function values at queried points; Bayesian optimization aims to locate the optimum using as few evaluations as possible [2][3][9].
- Surrogate model: A Gaussian process (GP) commonly serves as the surrogate; the GP produces a predictive mean and uncertainty (e.g., a 95% confidence interval) for the objective at unobserved inputs [1][3].
- Acquisition function: An acquisition function (examples include Expected Improvement (EI) and Gaussian Process Upper Confidence Bound (UCB)) converts the surrogate’s predictions and uncertainties into a single score for each candidate point, guiding where to evaluate next to balance exploration (reducing uncertainty) and exploitation (improving the objective) [1][4].
- Procedure (high level): 
  - Start with initial (often random) evaluations to train the surrogate [1]. 
  - Fit/update the GP using observed input–output pairs to obtain mean and uncertainty predictions [3]. 
  - Optimize the acquisition function to pick the next point to evaluate (this internal optimization is typically inexpensive relative to the objective) [1][4]. 
  - Evaluate the true objective at that point, add the result to the data, and repeat until budget is exhausted or convergence [1][2][6].
- Advantages: Uses all prior evaluation information rather than local gradients; can find optima of nonconvex/multimodal black‑box functions with relatively few expensive evaluations, at the cost of extra computation to choose each next point [4].
- Applications and variations: Commonly used for hyperparameter tuning and other expensive optimization tasks; variants include using Bayesian models for pairwise comparisons and other specialized feedback types [2][8][6].

References
[1] Kagglebook, Page: 285
[2] Bayesianoptimizationinaction, Page: 29
[3] Bayesianoptimizationinaction, Page: 49
[4] Machinelearningalgorithmsindepth, Page: 169
[6] Interpretablemachinelearningwithpython, Page: 516
[8] Bayesianoptimizationinaction, Page: 283
[9] Deepreinforcementlearninghands on Three Edition, Page: 521","Bayesian Optimization (BayesOpt) is a strategy for optimising an expensive, black-box function when you can sample it but not see its formula or gradients, and you only get a small evaluation budget.
You already have the core idea: it uses a probabilistic model to choose evaluation points intelligently instead of randomly. Let’s flesh that out a bit.

The basic loop
Bayesian Optimisation works in a repeat-until-budget-runs-out loop:


Fit a surrogate model
Use the points you’ve already evaluated, (xi,f(xi))(x_i, f(x_i))(xi​,f(xi​)), to fit a probabilistic model of the unknown function.
Most commonly:


Gaussian Processes (GPs) for low-dimensional, continuous spaces


Or alternatives like random forests, TPE, Bayesian neural nets, etc.


This gives you, for every candidate xxx, a distribution over f(x)f(x)f(x), not just a single guess: a mean μ(x)\mu(x)μ(x) and uncertainty σ(x)\sigma(x)σ(x).


Define an acquisition function
The acquisition function uses μ(x)\mu(x)μ(x) and σ(x)\sigma(x)σ(x) to quantify how “promising” each point is. Popular choices:


Expected Improvement (EI)


Upper Confidence Bound (UCB)


Probability of Improvement (PI)


These naturally trade off:


Exploitation – sampling where the model thinks values are high


Exploration – sampling where uncertainty is large




Optimise the acquisition function
Find the xxx that maximises the acquisition function (much cheaper than evaluating the true black-box).
This gives the next point to evaluate with the real expensive function.


Evaluate the black-box at that point
Run the actual experiment / simulation / training job to get f(xnext)f(x_{\text{next}})f(xnext​).


Update the surrogate model
Add the new data point, refit/updated the model, and repeat.


Because the surrogate has a sense of both where looks good and where we’re ignorant, the optimiser can:


Avoid getting stuck forever in a local optimum


Focus evaluations where they’re most informative


Use very few evaluations to find something close to the global optimum


In your 1D example [−5,5][-5, 5][−5,5] with 10 evaluations, BayesOpt can zoom in near the true maximum (say around 4.3) instead of wasting samples uniformly like random search.

Why it’s useful in practice
Bayesian Optimisation shines when:


Evaluations are expensive
e.g. training a model for hours, running a CFD simulation, tuning a lab experiment.


No gradients or closed form
It treats the objective as a complete black box.


You care about global rather than local optima
The exploration–exploitation balance helps avoid getting trapped early.


Typical real-world uses include:


Hyperparameter tuning (learning rate, depth, regularisation, etc.)


Experimental design in science & engineering


Tuning compiler flags, system parameters, ad strategies, etc.



A few extra nuances


It can be extended to batch settings (choose several points to evaluate in parallel) and multi-objective optimisation (optimising trade-offs between several metrics).


It struggles as dimension gets very high (say > 30–50), so people combine it with dimensionality reduction, structure priors, or manual narrowing of the search space.


You can include constraints (e.g. only sample where latency < X ms) by modelling feasibility with its own probabilistic model.


In short: Bayesian Optimisation builds and constantly refines a probabilistic map of your unknown function, then uses that map to decide the most informative next place to look, squeezing as much value as possible from every evaluation.",0.5416666666666666,0.9861111110987848,0.896551724137931,0.7565010187458674
